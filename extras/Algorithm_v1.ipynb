{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "import time\r\n",
    "import argparse\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib.gridspec import GridSpec\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "import librosa\r\n",
    "import librosa.display\r\n",
    "import IPython.display as ipd\r\n",
    "from scipy.signal import wiener\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "\r\n",
    "filename_1 = 'audio_files/videoplayback.wav'\r\n",
    "filename_2 = 'audio_files/videoplayback2.m4a'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def ArgParser():\r\n",
    "    parser = argparse.ArgumentParser()\r\n",
    "    \r\n",
    "    parser.add_argument(\"--sample_rate\", dest=\"sample_rate\", type=int, default=16000)\r\n",
    "    parser.add_argument(\"--n_fft\", dest=\"n_fft\", type=int, default=2048)\r\n",
    "    parser.add_argument(\"--window_size\", dest=\"window_size\", type=int, default=400) # 25ms\r\n",
    "    parser.add_argument(\"--hop_length\", dest=\"hop_length\", type=int, default=160) # 10ms\r\n",
    "    parser.add_argument(\"--n_mels\", dest=\"n_mels\", type=int, default=64)\r\n",
    "    parser.add_argument(\"--n_mfcc\", dest=\"n_mfcc\", type=int, default=13)\r\n",
    "    parser.add_argument(\"--max_samples\", dest=\"max_samples\", type=int, default=64000)\r\n",
    "    parser.add_argument(\"--delta_width\", dest=\"delta_width\", type=int, default=3)\r\n",
    "    parser.add_argument(\"--learning_rate\", dest=\"learning_rate\", type=float, default=1e-3)\r\n",
    "    parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, default=50)\r\n",
    "    parser.add_argument(\"--dropout\", dest=\"dropout\", type=float, default=0.5)\r\n",
    "    parser.add_argument(\"--type\", dest=\"type\", type=str, default=\"mfcc\")\r\n",
    "    parser.add_argument(\"--main_dir\", dest='main_dir', type=str, default=\"Datasets/TIMIT-dataset/tfrec_data\")\r\n",
    "    \r\n",
    "    args = parser.parse_known_args()[0]\r\n",
    "    seq_len = int(np.ceil(args.max_samples / args.hop_length))\r\n",
    "    if args.type == \"mel\":\r\n",
    "        input_shape = (seq_len, args.n_mels, 1)\r\n",
    "    else:\r\n",
    "        input_shape = (seq_len, (args.n_mfcc * 3) + 2, 1)\r\n",
    "    parser.add_argument(\"--input_shape\", type=tuple, default=input_shape)\r\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=seq_len)\r\n",
    "    return parser.parse_known_args()[0]\r\n",
    "\r\n",
    "args = ArgParser()\r\n",
    "args"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(delta_width=3, dropout=0.5, epochs=50, hop_length=160, input_shape=(400, 41, 1), learning_rate=0.001, main_dir='Datasets/TIMIT-dataset/tfrec_data', max_samples=64000, n_fft=2048, n_mels=64, n_mfcc=13, sample_rate=16000, seq_len=400, type='mfcc', window_size=400)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Algorithm:\r\n",
    "    def __init__(self):\r\n",
    "        self.args = args\r\n",
    "        self.data = \r\n",
    "\r\n",
    "    def AudioProcessing(self, wav_path):\r\n",
    "\r\n",
    "\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def stream(block_size, data, sr, duration):\r\n",
    "    block_start = np.arange(0, int(duration), step=block_size) * sr\r\n",
    "    block_end = block_start[1::]\r\n",
    "    for i, j in zip(block_start, block_end):\r\n",
    "        y_block = data[i:j]\r\n",
    "        yield y_block\r\n",
    "\r\n",
    "def extract_phonemes(voiced_flag):\r\n",
    "    word, words = [], []\r\n",
    "    for i, j in enumerate(voiced_flag):\r\n",
    "        if j == True:\r\n",
    "            word.append(i)\r\n",
    "        else:\r\n",
    "            if word != []:\r\n",
    "                words.append(word)\r\n",
    "            word = []\r\n",
    "    return words\r\n",
    "\r\n",
    "def extract_stats(phonemes, times, f0):\r\n",
    "    stats = {}\r\n",
    "    idx, mean, std = [], [], []\r\n",
    "    for i, phoneme in enumerate(phonemes):\r\n",
    "        mag = [f0[idx] for idx in phoneme]\r\n",
    "        mean.append(np.mean(mag))\r\n",
    "        std.append(np.std(mag))\r\n",
    "        idx.append(i)\r\n",
    "    stats['index'] = idx\r\n",
    "    stats['mean'] = mean\r\n",
    "    stats['std'] = std\r\n",
    "    return stats\r\n",
    "\r\n",
    "def frames_to_samples(phoneme, hop_length):\r\n",
    "    start_idx = librosa.frames_to_samples(\r\n",
    "        phoneme[0], hop_length=hop_length, n_fft=2048)\r\n",
    "    end_idx = librosa.frames_to_samples(\r\n",
    "        phoneme[-1], hop_length=hop_length, n_fft=2048)\r\n",
    "    return start_idx, end_idx\r\n",
    "\r\n",
    "def Segmentor(args):\r\n",
    "    spectrogram = Input(shape=args.input_shape, dtype=tf.float32, name='audio')\r\n",
    "    mask = Input(shape=args.input_shape[0], dtype=tf.bool, name='mask')\r\n",
    "\r\n",
    "    x = TimeDistributed(Conv1D(128, 3))(spectrogram)\r\n",
    "    x = TimeDistributed(ReLU())(x)\r\n",
    "    x = TimeDistributed(BatchNormalization())(x)\r\n",
    "    x = TimeDistributed(MaxPool1D(2))(x)\r\n",
    "\r\n",
    "    x = TimeDistributed(Conv1D(64, 3))(x)\r\n",
    "    x = TimeDistributed(ReLU())(x)\r\n",
    "    x = TimeDistributed(BatchNormalization())(x)\r\n",
    "    x = TimeDistributed(MaxPool1D(2))(x)\r\n",
    "\r\n",
    "    x = TimeDistributed(Conv1D(32, 3))(x)\r\n",
    "    x = TimeDistributed(ReLU())(x)\r\n",
    "    x = TimeDistributed(BatchNormalization())(x)\r\n",
    "    x = TimeDistributed(MaxPool1D(2))(x)\r\n",
    "    \r\n",
    "    x = TimeDistributed(Flatten())(x)\r\n",
    "    x = Bidirectional(LSTM(100, dropout=args.dropout, return_sequences=True))(x, mask=mask)\r\n",
    "    x = Bidirectional(LSTM(25, dropout=args.dropout, return_sequences=True))(x, mask=mask)\r\n",
    "    x = TimeDistributed(Dense(2, activation='softmax'))(x, mask=mask)\r\n",
    "    model = Model(inputs=[spectrogram, mask], outputs=x, name='Segmentor')\r\n",
    "    model.load_weights(\"model_weights\\segmentor.h5\")\r\n",
    "    return model\r\n",
    "\r\n",
    "def Inference(y, sr):\r\n",
    "    # Get attention mask\r\n",
    "    n_frames = len(y) // args.hop_length\r\n",
    "    pad_length = args.seq_len - n_frames \r\n",
    "    mask = tf.concat([tf.ones([n_frames]), tf.zeros([pad_length])], axis=-1)\r\n",
    "    mask = tf.cast(mask, dtype=tf.bool)\r\n",
    "    mask = tf.expand_dims(mask, axis=0)\r\n",
    "\r\n",
    "    # Get mfcc\r\n",
    "    y = librosa.util.fix_length(y, args.max_samples)\r\n",
    "    mfcc = librosa.feature.mfcc(\r\n",
    "        y=y, sr=sr, n_mfcc=args.n_mfcc, hop_length=args.hop_length,\r\n",
    "        win_length=args.window_size, n_mels=args.n_mels,\r\n",
    "        n_fft=args.n_fft, fmin=0, fmax=8000)[:, :args.seq_len]\r\n",
    "    mfcc = np.transpose(mfcc)\r\n",
    "    delta = librosa.feature.delta(\r\n",
    "        mfcc, width=args.delta_width, order=1, axis=0)\r\n",
    "    delta2 = librosa.feature.delta(\r\n",
    "        mfcc, width=args.delta_width, order=2, axis=0)\r\n",
    "    zcr = librosa.feature.zero_crossing_rate(\r\n",
    "        y=y, frame_length=args.window_size, hop_length=args.hop_length)\r\n",
    "    zcr =  np.transpose(zcr)[:-1, :]\r\n",
    "    f0 = librosa.yin(y=y, sr=sr, fmin=50, fmax=2000, win_length=args.window_size,\r\n",
    "        hop_length=args.hop_length, frame_length=2048)\r\n",
    "    f0 = np.expand_dims(np.diff(f0, axis=-1), axis=-1)\r\n",
    "    mfcc = np.concatenate((mfcc, delta, delta2, zcr, f0), axis=-1)\r\n",
    "    mfcc = tf.convert_to_tensor(mfcc, dtype=tf.float32)\r\n",
    "    mfcc = tf.expand_dims(mfcc, axis=0)\r\n",
    "\r\n",
    "    # Infer\r\n",
    "    inputs = {\"audio\": mfcc, \"mask\": mask}\r\n",
    "    model = Segmentor(args)\r\n",
    "    y_pred = model.predict(inputs)[0]\r\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\r\n",
    "    y_pred = tf.squeeze(np.where(y_pred==1))\r\n",
    "    y_pred = [(y_pred[i].numpy(), y_pred[i+1].numpy()) for i, b in enumerate(y_pred) if i < len(y_pred) - 1]\r\n",
    "\r\n",
    "    # Post-processing\r\n",
    "    y_pred = [frames for frames in y_pred if (frames[1] - frames[0]) >= 2]   \r\n",
    "    return y_pred\r\n",
    "\r\n",
    "def Plotter(filename, block_size, block):\r\n",
    "    \r\n",
    "    \r\n",
    "    \r\n",
    "\r\n",
    "class Algorithm():\r\n",
    "    def __init__(self, filename, block_size):\r\n",
    "        self.data, self.sr = librosa.load(\r\n",
    "            filename, sr=librosa.get_samplerate(filename))\r\n",
    "        self.y_pred\r\n",
    "        self.duration = librosa.get_duration(self.data, sr=self.sr)\r\n",
    "        self.fmin = librosa.note_to_hz('G2')\r\n",
    "        self.fmax = librosa.note_to_hz('A4')\r\n",
    "        self.frame_length = 2048\r\n",
    "        self.hop_length = self.frame_length // 4\r\n",
    "        self.block_size = block_size\r\n",
    "        self.n_blocks = int(self.duration // self.block_size)\r\n",
    "        self.residual = 0\r\n",
    "        self.benchmark = 0\r\n",
    "        self.forward = 0\r\n",
    "        self.mag_rate = 0.1\r\n",
    "        self.grad_limit = 1.0\r\n",
    "        self.mono_block = False\r\n",
    "        self.model = Segmentor(args)\r\n",
    "    \r\n",
    "    def train(self):\r\n",
    "        false_counter, true_counter = 0, 0\r\n",
    "        blocks = []\r\n",
    "        for i, y_block in enumerate(\r\n",
    "                stream(block_size=self.block_size, data=self.data,\r\n",
    "                       sr=self.sr, duration=self.duration)):          \r\n",
    "\r\n",
    "            total = 0       \r\n",
    "            start_time = time.time()\r\n",
    "            f0, voiced_flag, voiced_prob = librosa.pyin(\r\n",
    "                    y_block, sr=self.sr, fmin=self.fmin, fmax=self.fmax,\r\n",
    "                    frame_length=self.frame_length, switch_prob=1e-10,\r\n",
    "                    no_trough_prob=1e-2, resolution=1e-1)\r\n",
    "                \r\n",
    "            zcr = librosa.feature.zero_crossing_rate(y_block, frame_length=self.frame_length*2)[0] \r\n",
    "            zcr = np.abs(np.gradient(zcr))\r\n",
    "            \r\n",
    "            rms = librosa.feature.rms(y_block)[0] \r\n",
    "\r\n",
    "            f0 = np.where(\r\n",
    "                (voiced_flag==True) & (rms<rms.mean()) & (zcr<0.2), \r\n",
    "                np.nan, f0)\r\n",
    "\r\n",
    "            voiced_flag = np.where(\r\n",
    "                (voiced_flag==True) & (rms<rms.mean()) & (zcr<0.2), \r\n",
    "                False, voiced_flag)\r\n",
    "\r\n",
    "            voiced_sum = voiced_flag.sum()\r\n",
    "\r\n",
    "            if voiced_sum == 0:\r\n",
    "                print(\"{}/{} - No speech detected.\".format(i+1, self.n_blocks))\r\n",
    "                false_counter += 1\r\n",
    "                pass\r\n",
    "            else:\r\n",
    "                phonemes = extract_phonemes(voiced_flag)\r\n",
    "                for j, phoneme in enumerate(phonemes):\r\n",
    "                    start_idx, end_idx = phoneme[0], phoneme[-1]\r\n",
    "                    length = end_idx - start_idx\r\n",
    "                    mean = np.mean(f0[start_idx:end_idx])\r\n",
    "                    delta = np.abs(mean - self.residual)\r\n",
    "                    if length > 1:\r\n",
    "                        grad = np.abs(np.mean(np.gradient(f0[start_idx:end_idx], edge_order=1)))\r\n",
    "                    else:\r\n",
    "                        grad = 1\r\n",
    "                    if self.forward == 0:\r\n",
    "                        self.buffer = len(phonemes) // 2\r\n",
    "                    if ((self.forward == 0) & (j < self.buffer)) | (delta > self.benchmark * 0.3):\r\n",
    "                        self.benchmark += (mean / self.buffer)\r\n",
    "                    else:\r\n",
    "                        if (self.benchmark * self.mag_rate > delta) & (grad < self.grad_limit):\r\n",
    "                            total += length\r\n",
    "                    self.residual = mean\r\n",
    "\r\n",
    "                self.forward = mean\r\n",
    "                voiced_ratio = voiced_sum / len(voiced_flag)\r\n",
    "                elapsed_time = time.time() - start_time\r\n",
    "                monotonic_ratio = total / voiced_sum\r\n",
    "                \r\n",
    "                if monotonic_ratio >= 0.4:\r\n",
    "                    if true_counter == 0:\r\n",
    "                        block_start = i * self.block_size\r\n",
    "                    else:\r\n",
    "                        false_counter = 0\r\n",
    "                    mono_block = True\r\n",
    "                    true_counter += 1\r\n",
    "                else:\r\n",
    "                    mono_block = False\r\n",
    "                    false_counter += 1\r\n",
    "                    if (false_counter >= 3) & (true_counter > 0): # Reset counters\r\n",
    "                        true_counter = 0 \r\n",
    "                        false_counter = 0\r\n",
    "                        blocks.append([block_start, i * self.block_size])\r\n",
    "\r\n",
    "                print(\"{}/{} - voiced_ratio: {:.2f}% - phonemes: {} - monotonic_frames: {} - monotonic_ratio: {:.2f}% - elapsed_time: {:.2f}s\".format(\r\n",
    "                    i+1, self.n_blocks, voiced_ratio*100, len(phonemes), total, monotonic_ratio*100, elapsed_time)) \r\n",
    "        \r\n",
    "        print(\"Mononotic blocks:\\n\", blocks)          "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "y, sr = librosa.load(filename_1, sr=None)\r\n",
    "if sr != args.sample_rate:\r\n",
    "    y = librosa.resample(y, orig_sr=sr, target_sr=args.sample_rate)\r\n",
    "    sr = args.sample_rate\r\n",
    "\r\n",
    "print(len(y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40724457\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Stream(block_size, data, sr, duration):\r\n",
    "    block_start = np.arange(0, int(duration), step=block_size) * sr\r\n",
    "    block_end = block_start[1::]\r\n",
    "    for i, j in zip(block_start, block_end):\r\n",
    "        y_block = data[i:j]\r\n",
    "        yield y_block"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Stream(frame_size, y, sr):"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Analysis\n",
    "## Monotone 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "outputs = Plotter(filename=filename_1, block_size=6, block=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40724457\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-c35845edd5af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPlotter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-d772d4f1bc88>\u001b[0m in \u001b[0;36mPlotter\u001b[1;34m(filename, block_size, block)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# if len(data) > 64000:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m#     for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mphoneme_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m     \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_duration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mfmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnote_to_hz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'G2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y, sr, phonemes, times, hop_length, f0 = plotter(filename=filename_1, block_size=6, block=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Block sample"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ipd.Audio(data=y, rate=sr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Phoneme sample"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if phonemes != []:\r\n",
    "    start_idx, end_idx = frames_to_samples(phonemes[0], hop_length)\r\n",
    "    ipd.display(ipd.Audio(y[start_idx:end_idx]*2, rate=sr, autoplay=True))\r\n",
    "else:\r\n",
    "    print(\"No phonemes detected.\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stats = extract_stats(phonemes, times, f0)\r\n",
    "sns.lineplot(x=stats['index'], y=stats['mean'], label='mean')\r\n",
    "sns.lineplot(x=stats['index'], y=stats['std'], label='std')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y, sr, phonemes, times, hop_length, f0 = plotter(filename=filename_2, block_size=6, block=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Block sample"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ipd.Audio(data=y, rate=sr)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Phoneme sample"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if phonemes != []:\r\n",
    "    start_idx, end_idx = frames_to_samples(phonemes[2], hop_length)\r\n",
    "    ipd.display(ipd.Audio(y[start_idx:end_idx]*2, rate=sr, autoplay=True))\r\n",
    "else:\r\n",
    "    print(\"No phonemes detected.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stats = extract_stats(phonemes, times, f0)\r\n",
    "sns.lineplot(x=stats['index'], y=stats['mean'], label='mean')\r\n",
    "sns.lineplot(x=stats['index'], y=stats['std'], label='std')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "$\\frac{voiced frames}{duration}$ = voiced ratio\n",
    "\n",
    "$\\frac{monotonic frames}{voiced frames}$ = monotonic ratio\n",
    "\n",
    "\n",
    "## Monotone (Monologue)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Algorithm(filename=filename_1, block_size=6).train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normal (Clip)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Algorithm(filename=filename_2, block_size=6).train()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b851d2923cfa3a2562599062e05fd9893d86a7c009c64d8ad3756552e4dd5f41"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tf-gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}